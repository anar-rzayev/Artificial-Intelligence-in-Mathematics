{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Sarsa and Q-learning.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xZovQ28fXjc"
      },
      "source": [
        "### SARSA and Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07CHf6XJfXje"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZQglBQ8LI5f"
      },
      "source": [
        "#### Make an environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TANF6C4YfXjg"
      },
      "source": [
        "# Make a maze\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = plt.gca()\n",
        "\n",
        "# draw red walls\n",
        "plt.plot([1, 1], [0, 1], color='red', linewidth=2)\n",
        "plt.plot([1, 2], [2, 2], color='red', linewidth=2)\n",
        "plt.plot([2, 2], [2, 1], color='red', linewidth=2)\n",
        "plt.plot([2, 3], [1, 1], color='red', linewidth=2)\n",
        "\n",
        "# denote states as 'S0'~'S8'\n",
        "plt.text(0.5, 2.5, 'S0', size=14, ha='center')\n",
        "plt.text(1.5, 2.5, 'S1', size=14, ha='center')\n",
        "plt.text(2.5, 2.5, 'S2', size=14, ha='center')\n",
        "plt.text(0.5, 1.5, 'S3', size=14, ha='center')\n",
        "plt.text(1.5, 1.5, 'S4', size=14, ha='center')\n",
        "plt.text(2.5, 1.5, 'S5', size=14, ha='center')\n",
        "plt.text(0.5, 0.5, 'S6', size=14, ha='center')\n",
        "plt.text(1.5, 0.5, 'S7', size=14, ha='center')\n",
        "plt.text(2.5, 0.5, 'S8', size=14, ha='center')\n",
        "plt.text(0.5, 2.3, 'START', ha='center')\n",
        "plt.text(2.5, 0.3, 'GOAL', ha='center')\n",
        "\n",
        "# set the range and remove ticks\n",
        "ax.set_xlim(0, 3)\n",
        "ax.set_ylim(0, 3)\n",
        "plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                labelbottom=False, right=False, left=False, labelleft=False)\n",
        "\n",
        "# Draw green circle on 'S0' to indicate the current state\n",
        "line, = ax.plot([0.5], [2.5], marker=\"o\", color='g', markersize=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cp6FrSPLPq-"
      },
      "source": [
        "#### Make a random policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXlSEYK-fXjh"
      },
      "source": [
        "# Set theta_0 for random policy\n",
        "\n",
        "# row denotes state 0~7, and column denotes an action direction (order: up,right,down,left)\n",
        "theta_0 = np.array([[np.nan, 1, 1, np.nan],  # s0\n",
        "                    [np.nan, 1, np.nan, 1],  # s1\n",
        "                    [np.nan, np.nan, 1, 1],  # s2\n",
        "                    [1, 1, 1, np.nan],  # s3\n",
        "                    [np.nan, np.nan, 1, 1],  # s4\n",
        "                    [1, np.nan, np.nan, np.nan],  # s5\n",
        "                    [1, np.nan, np.nan, np.nan],  # s6\n",
        "                    [1, 1, np.nan, np.nan],  # s7、※Since s8 is the goal(terminal state), we do not need the policy\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Iyuc3hfXji"
      },
      "source": [
        "# Make a random policy using theta_0\n",
        "\n",
        "def simple_convert_into_pi_from_theta(theta):\n",
        "\n",
        "    [m, n] = theta.shape\n",
        "    pi = np.zeros((m, n))\n",
        "    for i in range(0, m):\n",
        "        pi[i, :] = theta[i, :] / np.nansum(theta[i, :])  # calculate proportion\n",
        "\n",
        "    pi = np.nan_to_num(pi)  # change nan to 0\n",
        "\n",
        "    return pi\n",
        "\n",
        "# calculate a random policy pi_0\n",
        "pi_0 = simple_convert_into_pi_from_theta(theta_0)\n",
        "print(pi_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAA1kVXeKmun"
      },
      "source": [
        "#### **(1) SARSA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQQ_WKbJfXjj"
      },
      "source": [
        "# Initialize the action-value function Q\n",
        "\n",
        "[a, b] = theta_0.shape\n",
        "Q1 = np.random.rand(a, b) * theta_0 * 0.1 # change the value of impossible action to nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jkx9IvfK_fS"
      },
      "source": [
        "#### **(2) Q-learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyFWnCwLJZg0"
      },
      "source": [
        "# Initialize the action-value function Q\n",
        "\n",
        "Q2 = np.random.rand(a, b) * theta_0 * 0.1 # change the value of impossible action to nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBAZC47ZLVzF"
      },
      "source": [
        "#### Implement ε-greedy algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xYCA6UTfXjj"
      },
      "source": [
        "def get_action(s, Q, epsilon, pi_0):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "\n",
        "    # for the exploration, we take a random action with probability epsilon, \n",
        "    # and we take an action that maximizes the Q-value with probability 1-epsilon\n",
        "    if np.random.rand() < epsilon:\n",
        "        # take a random action\n",
        "        next_direction = np.random.choice(direction, p=pi_0[s, :])\n",
        "    else:\n",
        "        # take an action that maximizes the Q-value\n",
        "        next_direction = direction[np.nanargmax(Q[s, :])]\n",
        "\n",
        "    # change an action to an index\n",
        "    if next_direction == \"up\":\n",
        "        action = 0\n",
        "    elif next_direction == \"right\":\n",
        "        action = 1\n",
        "    elif next_direction == \"down\":\n",
        "        action = 2\n",
        "    elif next_direction == \"left\":\n",
        "        action = 3\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "def get_s_next(s, a):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "    next_direction = direction[a]\n",
        "\n",
        "    # decide the next state with an action\n",
        "    if next_direction == \"up\":\n",
        "        s_next = s - 3  # if we take a 'up' action, the state number decreases by 3\n",
        "    elif next_direction == \"right\":\n",
        "        s_next = s + 1  # if we take a 'right' action, the state number increases by 1\n",
        "    elif next_direction == \"down\":\n",
        "        s_next = s + 3  # if we take a 'down' action, the state number increases by 3\n",
        "    elif next_direction == \"left\":\n",
        "        s_next = s - 1  # if we take a 'left' action, the state number decreases by 1\n",
        "\n",
        "    return s_next"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZmXDGVNLdQ8"
      },
      "source": [
        "#### SARSA and Q-learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy5iosBHfXjk"
      },
      "source": [
        "# Modify the Q-value with SARSA algorithm\n",
        "\n",
        "def Sarsa(s, a, r, s_next, a_next, Q, eta, gamma):\n",
        "\n",
        "    if s_next == 8:  # when we reach the goal\n",
        "        Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n",
        "\n",
        "    else:\n",
        "        Q[s, a] = Q[s, a] + eta * (r + gamma * Q[s_next, a_next] - Q[s, a])\n",
        "\n",
        "    return Q\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M46gjJf18HQj"
      },
      "source": [
        "# Modify the Q-value with Q-learning algorithm\n",
        "\n",
        "def Q_learning(s, a, r, s_next, Q, eta, gamma):\n",
        "\n",
        "    if s_next == 8:  # when we reach the goal\n",
        "        Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n",
        "\n",
        "    else:\n",
        "        Q[s, a] = Q[s, a] + eta * (r + gamma * np.nanmax(Q[s_next,: ]) - Q[s, a])\n",
        "\n",
        "    return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_l0ZChdfXjl"
      },
      "source": [
        "# Make a function to escape the maze: it returns (state-action) history and the final Q-value\n",
        "\n",
        "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi, algorithm):\n",
        "    assert algorithm in {'SARSA', 'Sarsa', 'Q-learning'}, \"algorithm should be one of {'SARSA', 'Sarsa', 'Q-learning'}\"\n",
        "\n",
        "    s = 0  # initial state\n",
        "    a = a_next = get_action(s, Q, epsilon, pi)  # first action\n",
        "    s_a_history = [[0, np.nan]]  # history of states and actions\n",
        "\n",
        "    while (1):  \n",
        "        a = a_next  # decide next action\n",
        "\n",
        "        s_a_history[-1][1] = a\n",
        "        # append the current action to history\n",
        "\n",
        "        s_next = get_s_next(s, a)\n",
        "        # decide the next state\n",
        "\n",
        "        s_a_history.append([s_next, np.nan])\n",
        "        # append the next state to history (since we do not know the next action at this point, leave it as nan)\n",
        "\n",
        "        # get the reward and decide next action\n",
        "        if s_next == 8:\n",
        "            r = 1  \n",
        "            a_next = np.nan\n",
        "        else:\n",
        "            r = 0\n",
        "            a_next = get_action(s_next, Q, epsilon, pi)\n",
        "        \n",
        "        # modify the Q-function\n",
        "        if algorithm == 'SARSA' or algorithm == 'Sarsa':\n",
        "            Q = Sarsa(s, a, r, s_next, a_next, Q, eta, gamma)\n",
        "\n",
        "        if algorithm == 'Q-learning':\n",
        "            Q = Q_learning(s, a, r, s_next, Q, eta, gamma)\n",
        "\n",
        "        # decide whether the episode ends\n",
        "        if s_next == 8:  # terminate if we reach the goal\n",
        "            break\n",
        "        else:\n",
        "            s = s_next\n",
        "\n",
        "    return [s_a_history, Q]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4X5Z-sFLoZR"
      },
      "source": [
        "#### Escape the maze with SARSA algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6ki2hnTfXjm"
      },
      "source": [
        "eta = 0.1  # learning rate\n",
        "gamma = 0.9  # discount factor\n",
        "epsilon = 0.5  # initial epsilon value for epsilon-greedy algorithm\n",
        "v = np.nanmax(Q1, axis=1)  # find maximum Q-value for each state\n",
        "is_continue = True\n",
        "episode = 1\n",
        "\n",
        "V1 = []  # save the state-value for each episode\n",
        "V1.append(np.nanmax(Q1, axis=1))\n",
        "\n",
        "while is_continue:  \n",
        "    print(\"epsiode: \" + str(episode))\n",
        "\n",
        "    # decrease epsilon gradually\n",
        "    epsilon = epsilon / 2\n",
        "\n",
        "    [s_a_history, Q1] = goal_maze_ret_s_a_Q(Q1, epsilon, eta, gamma, pi_0, 'Sarsa')\n",
        "\n",
        "    # change the state-value function\n",
        "    new_v = np.nanmax(Q1, axis=1)  \n",
        "    print(np.sum(np.abs(new_v - v)))  # print how much the state-value function is changed\n",
        "    v = new_v\n",
        "    V1.append(v) # append the state-value at the end of current episode\n",
        "\n",
        "    print(\"The number of steps to reach the goal is \" + str(len(s_a_history) - 1))\n",
        "\n",
        "    # repeat 100 episodes\n",
        "    episode = episode + 1\n",
        "    if episode > 100:\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzai9PD_Lvj0"
      },
      "source": [
        "#### Escape the maze with Q-learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qzM28f9JutO"
      },
      "source": [
        "# Escape the maze\n",
        "\n",
        "eta = 0.1  # learning rate\n",
        "gamma = 0.9  # discount factor\n",
        "epsilon = 0.5  # initial epsilon value for epsilon-greedy algorithm\n",
        "v = np.nanmax(Q2, axis=1)  # find maximum Q-value for each state\n",
        "is_continue = True\n",
        "episode = 1\n",
        "\n",
        "V2 = []  # save the state-value for each episode\n",
        "V2.append(np.nanmax(Q2, axis=1))\n",
        "\n",
        "while is_continue:  \n",
        "    print(\"epsiode: \" + str(episode))\n",
        "\n",
        "    # decrease epsilon gradually\n",
        "    epsilon = epsilon / 2\n",
        "\n",
        "    [s_a_history, Q2] = goal_maze_ret_s_a_Q(Q2, epsilon, eta, gamma, pi_0, 'Q-learning')\n",
        "\n",
        "    # change the state-value function\n",
        "    new_v = np.nanmax(Q2, axis=1)  \n",
        "    print(np.sum(np.abs(new_v - v)))  # print how much the state-value function is changed\n",
        "    v = new_v\n",
        "    V2.append(v) # append the state-value at the end of current episode\n",
        "\n",
        "    print(\"The number of steps to reach the goal is \" + str(len(s_a_history) - 1))\n",
        "\n",
        "    # repeat 100 episodes\n",
        "    episode = episode + 1\n",
        "    if episode > 100:\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxvxlfoAEBvd"
      },
      "source": [
        "# Visualize the change of state-value\n",
        "# c.f.) URL http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-notebooks/\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "import matplotlib.cm as cm  # color map\n",
        "\n",
        "\n",
        "def init():\n",
        "    # initialize the color of background\n",
        "    line.set_data([], [])\n",
        "    return (line,)\n",
        "\n",
        "\n",
        "def animate1(i):\n",
        "    # draw a picture by frame\n",
        "    # color each state based on the state-value\n",
        "    line, = ax.plot([0.5], [2.5], marker=\"s\",\n",
        "                    color=cm.jet(V1[i][0]), markersize=85)  # S0\n",
        "    line, = ax.plot([1.5], [2.5], marker=\"s\",\n",
        "                    color=cm.jet(V1[i][1]), markersize=85)  # S1\n",
        "    line, = ax.plot([2.5], [2.5], marker=\"s\",\n",
        "                    color=cm.jet(V1[i][2]), markersize=85)  # S2\n",
        "    line, = ax.plot([0.5], [1.5], marker=\"s\",\n",
        "                    color=cm.jet(V1[i][3]), markersize=85)  # S3\n",
        "    line, = ax.plot([1.5], [1.5], marker=\"s\",\n",
        "                    color=cm.jet(V1[i][4]), markersize=85)  # S4\n",
        "    line, = ax.plot([2.5], [1.5], marker=\"s\",\n",
        "                    color=cm.jet(V1[i][5]), markersize=85)  # S5\n",
        "    line, = ax.plot([0.5], [0.5], marker=\"s\",\n",
        "                    color=cm.jet(V1[i][6]), markersize=85)  # S6\n",
        "    line, = ax.plot([1.5], [0.5], marker=\"s\",\n",
        "                    color=cm.jet(V1[i][7]), markersize=85)  # S7\n",
        "    line, = ax.plot([2.5], [0.5], marker=\"s\",\n",
        "                    color=cm.jet(1.0), markersize=85)  # S8\n",
        "    return (line,)\n",
        "\n",
        "\n",
        "# make an animation\n",
        "anim = animation.FuncAnimation(\n",
        "    fig, animate1, init_func=init, frames=len(V1), interval=200, repeat=False)\n",
        "\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBwkZIvKMW4q"
      },
      "source": [
        "def animate2(i):\n",
        "    # draw a picture by frame\n",
        "    # color each state based on the state-value\n",
        "    line, = ax.plot([0.5], [2.5], marker=\"s\",\n",
        "                    color=cm.jet(V2[i][0]), markersize=85)  # S0\n",
        "    line, = ax.plot([1.5], [2.5], marker=\"s\",\n",
        "                    color=cm.jet(V2[i][1]), markersize=85)  # S1\n",
        "    line, = ax.plot([2.5], [2.5], marker=\"s\",\n",
        "                    color=cm.jet(V2[i][2]), markersize=85)  # S2\n",
        "    line, = ax.plot([0.5], [1.5], marker=\"s\",\n",
        "                    color=cm.jet(V2[i][3]), markersize=85)  # S3\n",
        "    line, = ax.plot([1.5], [1.5], marker=\"s\",\n",
        "                    color=cm.jet(V2[i][4]), markersize=85)  # S4\n",
        "    line, = ax.plot([2.5], [1.5], marker=\"s\",\n",
        "                    color=cm.jet(V2[i][5]), markersize=85)  # S5\n",
        "    line, = ax.plot([0.5], [0.5], marker=\"s\",\n",
        "                    color=cm.jet(V2[i][6]), markersize=85)  # S6\n",
        "    line, = ax.plot([1.5], [0.5], marker=\"s\",\n",
        "                    color=cm.jet(V2[i][7]), markersize=85)  # S7\n",
        "    line, = ax.plot([2.5], [0.5], marker=\"s\",\n",
        "                    color=cm.jet(1.0), markersize=85)  # S8\n",
        "    return (line,)\n",
        "\n",
        "\n",
        "# make an animation\n",
        "anim = animation.FuncAnimation(\n",
        "    fig, animate2, init_func=init, frames=len(V2), interval=200, repeat=False)\n",
        "\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3NPaG_uNumi"
      },
      "source": [
        "### **Reference**\n",
        "Pytorch를 활용한 강화학습/심층강화학습 실전 입문"
      ]
    }
  ]
}